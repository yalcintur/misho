# Policy function finetuning
train_arguments:
  device: "cuda:1"
  model_name: "HuggingFaceTB/SmolLM2-135M"
  save_model_name: "smol-lm-135m"
  output_dir: "./sft_output"
  max_steps: 10
  per_device_train_batch_size: 2
  learning_rate: 5e-5
  logging_steps: 1
  save_steps: 10
  evaluation_strategy: "steps"
  eval_steps: 10
  max_seq_len: 1024
  use_mps_device: false
  hub_model_id: "smol-lm-135m"
  dataset_file: "/home/weh4401/st/misho/dataset/game24"